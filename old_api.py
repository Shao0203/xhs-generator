import os
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory


def get_chat_response(prompt, memory, api_key=os.getenv('KIMI_API_KEY')):
    llm = ChatOpenAI(
        model='kimi-k2-0711-preview',
        base_url='https://api.moonshot.cn/v1',
        api_key=api_key
    )
    chain = ConversationChain(llm=llm, memory=memory)

    response = chain.invoke({'input': prompt})
    return response['response']


memory = ConversationBufferMemory(return_messages=True)
print(get_chat_response('æˆ‘æ˜¯äº®äº®ï¼Œä½ æ˜¯ä»€ä¹ˆæ¨¡å‹?(å›ç­”20å­—ä»¥å†…)', memory))
print(get_chat_response('æˆ‘ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿä½ è®°å¾—æˆ‘åå­—å—?', memory))
# æˆ‘æ˜¯Kimiï¼Œç”±æœˆä¹‹æš—é¢è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹
# ä½ ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯â€œä½ æ˜¯ä»€ä¹ˆæ¨¡å‹?â€ï¼Œæˆ‘è®°å¾—ä½ å«äº®äº®ã€‚


# ---------- ä»¥ä¸‹æ˜¯streamlitå‰ç«¯ä»£ç  ----------


# import streamlit as st
# from langchain.memory import ConversationBufferMemory
# from chat import get_chat_response


# st.title('ğŸ’¬æ™ºèƒ½èŠå¤©åŠ©æ‰‹')

# if 'memory' not in st.session_state:
#     st.session_state.memory = ConversationBufferMemory(return_messages=True)
#     st.session_state.messages = [{'role': 'ai', 'content': 'ä½ å¥½, æˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹!'}]

# for message in st.session_state.messages:
#     st.chat_message(message['role']).write(message['content'])

# prompt = st.chat_input()

# if prompt:
#     st.session_state.messages.append({'role': 'human', 'content': prompt})
#     st.chat_message('human').write(prompt)

#     with st.spinner('AI is thinking...'):
#         response = get_chat_response(prompt, st.session_state.memory)

#     st.session_state.messages.append({'role': 'ai', 'content': response})
#     st.chat_message('ai').write(response)
